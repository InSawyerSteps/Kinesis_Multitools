{
    "pattern_name": "incremental_vector_indexing",
    "description": "Efficiently indexes project files for semantic search by reusing unchanged vectors and only embedding changed/added files. Handles file deletions and updates metadata atomically.",
    "source_file": "c:\\Projects\\MCP Server\\src\\toolz.py",
    "function_name": "index_project_files",
    "source_code": "def index_project_files(project_name: str, subfolder: Optional[str] = None, max_file_size_mb: int = 5) -> dict:\n    \"\"\"\n    Index the project for semantic search **incrementally**.\n\n    This version detects file additions, modifications, and deletions so that we **avoid\n    recomputing embeddings for every chunk on every run**.  It works by:\n\n    1. Loading any existing FAISS index and accompanying ``metadata.json``.\n    2. Collecting current project files (respecting ignore-lists / size / extension filters).\n    3. Comparing each file's *mtime* and *size* against what was stored during the last\n       indexing pass.\n       * Unchanged  \u2192  re-use the already-stored vectors (no embedding cost).\n       * Added / modified \u2192 re-chunk + embed just those files.\n       * Deleted \u2192 their vectors are dropped.\n    4. Re-building a **fresh** FAISS index from the union of reused + new vectors.  Using a\n       fresh index keeps the logic simple and avoids tricky in-place deletions while still\n       saving the heavy embedding cost.\n\n    NOTE: Each chunk entry in ``metadata.json`` now contains::\n\n        {\n          \"path\": \"relative/path/to/file.py\",\n          \"content\": \"<chunk text>\",\n          \"vector\": [..float32 list..],\n          \"file_mtime\": 1719690000.123,\n          \"file_size\": 2048\n        }\n\n    Args:\n        project_name (str): Name of the project as defined in ``PROJECT_ROOTS``.\n        subfolder (str, optional): If set, only index this subfolder within the project.\n        max_file_size_mb (int, optional): Maximum file size (in MB) to include (default ``5``).\n\n    Returns:\n        dict: {\n            'status': 'success' | 'error',\n            'message': str,\n            'files_scanned_and_included': int,   # Total files considered this run\n            'unchanged_files': int,              # Re-used without re-embedding\n            'updated_files': int,                # Added or modified & re-embedded\n            'deleted_files': int,                # Removed from index\n            'total_chunks_indexed': int,\n            'indexing_duration_seconds': float\n        }\n    \"\"\"\n    if not LIBS_AVAILABLE:\n        return {\"status\": \"error\", \"message\": \"Indexing failed: Required libraries (faiss, numpy, sentence-transformers) are not installed.\"}\n\n    # ------------------------------------------------------------\n    # Config & Setup\n    # ------------------------------------------------------------\n    directories_to_ignore = {\n        'node_modules', '.git', '__pycache__', 'venv', '.venv', 'target',\n        'build', 'dist', '.cache', '.idea', '.vscode', 'eggs', '.eggs'\n    }\n    text_extensions_to_include = {\n        '.py', '.js', '.ts', '.jsx', '.tsx', '.md', '.json', '.yaml', '.yml',\n        '.html', '.css', '.scss', '.txt', '.sh', '.bat', '.ps1', '.xml', '.rb',\n        '.java', '.c', '.h', '.cpp', '.go', '.rs', '.php'\n    }\n    max_file_size_bytes = max_file_size_mb * 1024 * 1024\n\n    CHUNK_SIZE = 512\n    CHUNK_OVERLAP = 64\n    BATCH_SIZE = 128\n\n    start_time = time.monotonic()\n\n    project_path = _get_project_path(project_name)\n    if not project_path:\n        return {\"status\": \"error\", \"message\": f\"Project '{project_name}' not found.\"}\n\n    scan_path = project_path / subfolder if subfolder else project_path\n    if subfolder and not scan_path.is_dir():\n        return {\"status\": \"error\", \"message\": f\"Subfolder '{subfolder}' not found in project.\"}\n\n    index_path = scan_path / INDEX_DIR_NAME\n    index_path.mkdir(exist_ok=True)\n\n    # ------------------------------------------------------------\n    # Step 1.  Gather current relevant files\n    # ------------------------------------------------------------\n    relevant_files: list[pathlib.Path] = [\n        p for p in scan_path.rglob('*') if p.is_file() and\n        not any(ignored in p.parts for ignored in directories_to_ignore) and\n        p.suffix.lower() in text_extensions_to_include and\n        p.stat().st_size <= max_file_size_bytes and\n        \".windsurf_search_index\" not in str(p) and not str(p).endswith(\".json\")\n    ]\n    logger.info(\"[index] Found %d relevant files to consider.\", len(relevant_files))\n\n    # ------------------------------------------------------------\n    # Step 2.  Load previous metadata (if any)\n    # ------------------------------------------------------------\n    old_metadata_file = index_path / \"metadata.json\"\n    old_metadata: list[dict] = []\n    if old_metadata_file.exists():\n        try:\n            with open(old_metadata_file, \"r\", encoding=\"utf-8\") as f:\n                old_metadata = json.load(f)\n            logger.info(\"[index] Loaded previous metadata with %d chunks.\", len(old_metadata))\n        except Exception as e:\n            logger.warning(\"[index] Failed to load existing metadata.json: %s. A full re-index will be performed.\", e)\n            old_metadata = []\n\n    # Helper: map file path \u279c first chunk entry (to inspect stored stats)\n    old_stats_by_path: dict[str, dict] = {}\n    for entry in old_metadata:\n        path_key = entry.get(\"path\")\n        if path_key and path_key not in old_stats_by_path:\n            old_stats_by_path[path_key] = entry\n\n    # Current file stats lookup\n    current_stats: dict[str, tuple] = {}\n    for fp in relevant_files:\n        stat = fp.stat()\n        current_stats[str(fp.relative_to(project_path))] = (stat.st_mtime, stat.st_size, fp)\n\n    # ------------------------------------------------------------\n    # Step 3.  Categorise files\n    # ------------------------------------------------------------\n    unchanged_paths: set[str] = set()\n    updated_paths: set[str] = set()\n    for rel_path, (mtime, size, _fp) in current_stats.items():\n        old = old_stats_by_path.get(rel_path)\n        if old and old.get(\"file_mtime\") == mtime and old.get(\"file_size\") == size and \"vector\" in old:\n            unchanged_paths.add(rel_path)\n        else:\n            updated_paths.add(rel_path)\n    deleted_paths: set[str] = set(old_stats_by_path.keys()) - set(current_stats.keys())\n\n    logger.info(\n        \"[index] unchanged=%d updated=%d deleted=%d\",\n        len(unchanged_paths), len(updated_paths), len(deleted_paths)\n    )\n\n    # ------------------------------------------------------------\n    # Step 4.  Reuse vectors for unchanged chunks\n    # ------------------------------------------------------------\n    new_metadata: list[dict] = []\n    all_vectors: list[list[float]] = []\n    for entry in old_metadata:\n        if entry.get(\"path\") in unchanged_paths and \"vector\" in entry:\n            new_metadata.append(entry)\n            all_vectors.append(entry[\"vector\"])\n\n    # ------------------------------------------------------------\n    # Step 5.  Process updated (new/modified) files\n    # ------------------------------------------------------------\n    batch_texts: list[str] = []\n    batch_meta: list[tuple[str, int, float, int]] = []  # (rel_path, offset, mtime, size)\n\n    for rel_path in updated_paths:\n        fp = current_stats[rel_path][2]\n        try:\n            text = fp.read_text(\"utf-8\", errors=\"ignore\")\n            if not text.strip():\n                continue\n            mtime, size, _ = current_stats[rel_path]\n            for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP):\n                chunk_content = text[i : i + CHUNK_SIZE]\n                batch_texts.append(chunk_content)\n                batch_meta.append((rel_path, i, mtime, size))\n        except Exception as e:\n            logger.warning(\"[index] Could not read or chunk file %s: %s\", fp, e)\n\n    # Embed in batches\n    for i in range(0, len(batch_texts), BATCH_SIZE):\n        chunk_batch = batch_texts[i : i + BATCH_SIZE]\n        vectors = _embed_batch(chunk_batch)\n        for vec, (rel_path, _offset, mtime, size) in zip(vectors, batch_meta[i : i + BATCH_SIZE]):\n            new_metadata.append({\n                \"path\": rel_path,\n                \"content\": chunk_batch.pop(0),  # pop to keep memory low\n                \"vector\": vec,\n                \"file_mtime\": mtime,\n                \"file_size\": size,\n            })\n            all_vectors.append(vec)\n        logger.info(\"[index] Embedded batch of %d chunks. Total chunks so far: %d\", len(vectors), len(all_vectors))\n\n    if not all_vectors:\n        return {\"status\": \"error\", \"message\": \"No chunks available to build index (all files empty or unreadable).\"}\n\n    # ------------------------------------------------------------\n    # Step 6.  Build & save FAISS index + metadata\n    # ------------------------------------------------------------\n    try:\n        dim = len(all_vectors[0])\n        index = faiss.IndexFlatL2(dim)\n        index.add(np.array(all_vectors, dtype=np.float32))\n\n        faiss.write_index(index, str(index_path / \"index.faiss\"))\n        with open(old_metadata_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(new_metadata, f)\n    except Exception as e:\n        logger.exception(\"[index] Failed to build or save index: %s\", e)\n        return {\"status\": \"error\", \"message\": f\"Failed to build or save index: {e}\"}\n\n    duration = time.monotonic() - start_time\n    return {\n        \"status\": \"success\",\n        \"message\": f\"Project '{project_name}' indexed incrementally.\",\n        \"files_scanned_and_included\": len(relevant_files),\n        \"unchanged_files\": len(unchanged_paths),\n        \"updated_files\": len(updated_paths),\n        \"deleted_files\": len(deleted_paths),\n        \"total_chunks_indexed\": len(all_vectors),\n        \"indexing_duration_seconds\": round(duration, 2),\n    }",
    "added_at_utc": "2025-07-09T04:18:19Z"
}